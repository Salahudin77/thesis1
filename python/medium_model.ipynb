{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salahudin77/thesis/blob/main/python/medium_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import psutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import datetime\n",
        "import platform\n",
        "import statistics\n",
        "from pathlib import Path\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MemoryTracker:\n",
        "    def __init__(self, interval_ms=100):\n",
        "        self.interval_ms = interval_ms\n",
        "        self.memory_stats = []\n",
        "        self.running = False\n",
        "        self.thread = None\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "        self.peak_heap_usage = 0\n",
        "        self.peak_virtual_usage = 0\n",
        "        \n",
        "    def collect_stats(self):\n",
        "        process = psutil.Process(os.getpid())\n",
        "        stats = {\n",
        "            \"timestamp\": time.time() * 1000,  # ms since epoch\n",
        "            \"rss_bytes\": process.memory_info().rss,\n",
        "            \"vms_bytes\": process.memory_info().vms,\n",
        "            \"heap_used_mb\": process.memory_info().rss / 1024 / 1024,\n",
        "            \"virtual_used_mb\": process.memory_info().vms / 1024 / 1024\n",
        "        }\n",
        "        \n",
        "        self.peak_heap_usage = max(self.peak_heap_usage, stats[\"rss_bytes\"])\n",
        "        self.peak_virtual_usage = max(self.peak_virtual_usage, stats[\"vms_bytes\"])\n",
        "        \n",
        "        return stats\n",
        "    \n",
        "    def stats_collector(self):\n",
        "        import threading\n",
        "        while self.running:\n",
        "            self.memory_stats.append(self.collect_stats())\n",
        "            time.sleep(self.interval_ms / 1000)  # Convert ms to seconds\n",
        "    \n",
        "    def start(self):\n",
        "        if not self.running:\n",
        "            self.running = True\n",
        "            self.start_time = time.time()\n",
        "            import threading\n",
        "            self.thread = threading.Thread(target=self.stats_collector)\n",
        "            self.thread.daemon = True\n",
        "            self.thread.start()\n",
        "    \n",
        "    def stop(self):\n",
        "        if self.running:\n",
        "            self.running = False\n",
        "            self.end_time = time.time()\n",
        "            if self.thread:\n",
        "                self.thread.join(timeout=0.5)\n",
        "                self.thread = None\n",
        "    \n",
        "    def get_peak_heap_usage_bytes(self):\n",
        "        return self.peak_heap_usage\n",
        "    \n",
        "    def get_peak_heap_usage_mb(self):\n",
        "        return self.peak_heap_usage / 1024 / 1024\n",
        "    \n",
        "    def get_peak_virtual_usage_bytes(self):\n",
        "        return self.peak_virtual_usage\n",
        "    \n",
        "    def get_peak_virtual_usage_mb(self):\n",
        "        return self.peak_virtual_usage / 1024 / 1024\n",
        "    \n",
        "    def get_total_peak_memory_mb(self):\n",
        "        # In Python, we can't cleanly separate heap vs non-heap like in Java\n",
        "        # Instead, we'll use RSS as heap and the difference between VMS and RSS as \"non-heap\"\n",
        "        return self.get_peak_heap_usage_mb()\n",
        "    \n",
        "    def get_summary(self):\n",
        "        summary = {\n",
        "            \"sample_count\": len(self.memory_stats),\n",
        "            \"sample_interval_ms\": self.interval_ms,\n",
        "            \"peak_heap_mb\": self.get_peak_heap_usage_mb(),\n",
        "            \"peak_virtual_mb\": self.get_peak_virtual_usage_mb(),\n",
        "            \"peak_total_mb\": self.get_total_peak_memory_mb()\n",
        "        }\n",
        "        \n",
        "        if self.start_time and self.end_time:\n",
        "            summary[\"duration_ms\"] = (self.end_time - self.start_time) * 1000\n",
        "        \n",
        "        return summary\n",
        "    \n",
        "    def get_time_series_data(self):\n",
        "        if not self.memory_stats:\n",
        "            return []\n",
        "        \n",
        "        base_time = self.memory_stats[0][\"timestamp\"]\n",
        "        \n",
        "        return [{\n",
        "            \"time_ms\": stats[\"timestamp\"] - base_time,\n",
        "            \"heap_mb\": stats[\"heap_used_mb\"],\n",
        "            \"virtual_mb\": stats[\"virtual_used_mb\"]\n",
        "        } for stats in self.memory_stats]\n",
        "    \n",
        "    def __enter__(self):\n",
        "        self.start()\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.stop()\n",
        "\n",
        "\n",
        "class InferenceResult:\n",
        "    def __init__(self, prediction, time_ms, memory_used_mb, cpu_time_ms, peak_memory_mb, memory_profile):\n",
        "        self.prediction = prediction\n",
        "        self.time_ms = time_ms\n",
        "        self.memory_used_mb = memory_used_mb\n",
        "        self.cpu_time_ms = cpu_time_ms\n",
        "        self.peak_memory_mb = peak_memory_mb\n",
        "        self.memory_profile = memory_profile\n",
        "\n",
        "\n",
        "class TrainingResult:\n",
        "    def __init__(self, training_time_ms, peak_memory_mb, accuracy, memory_profile, evaluation):\n",
        "        self.training_time_ms = training_time_ms\n",
        "        self.peak_memory_mb = peak_memory_mb\n",
        "        self.accuracy = accuracy\n",
        "        self.memory_profile = memory_profile\n",
        "        self.evaluation = evaluation\n",
        "\n",
        "\n",
        "def create_medium_network():\n",
        "    \"\"\"Create a neural network matching the Java implementation's structure\"\"\"\n",
        "    # Medium complexity network with two hidden layers - identical to Java version\n",
        "    model = models.Sequential([\n",
        "        layers.Flatten(input_shape=(28, 28)),\n",
        "        layers.Dense(128, activation='relu'),  # First hidden layer with 128 neurons\n",
        "        layers.Dense(64, activation='relu'),   # Second hidden layer with 64 neurons\n",
        "        layers.Dense(10, activation='softmax') # Output layer\n",
        "    ])\n",
        "    \n",
        "    # Use Adam optimizer with identical learning rate\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def train_network_with_metrics(model):\n",
        "    \"\"\"Train the network and measure performance metrics like the Java version\"\"\"\n",
        "    # Configuration - matching Java parameters\n",
        "    batch_size = 256  # Match Java batch size\n",
        "    num_epochs = 10   # Match Java epochs\n",
        "    \n",
        "    # Force garbage collection before starting\n",
        "    gc.collect()\n",
        "    time.sleep(0.1)  # Small delay like Java's Thread.sleep(100)\n",
        "    \n",
        "    # Set up memory tracker with 100ms intervals (same as Java)\n",
        "    with MemoryTracker(interval_ms=100) as memory_tracker:\n",
        "        start_training_time = time.time()\n",
        "        \n",
        "        # Load and preprocess MNIST dataset\n",
        "        print(\"Loading data...\")\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "        \n",
        "        # Create progress callback similar to ScoreIterationListener in Java\n",
        "        class ScoreLogger(tf.keras.callbacks.Callback):\n",
        "            def on_batch_end(self, batch, logs=None):\n",
        "                if batch % 100 == 0:\n",
        "                    print(f\"Batch {batch}, loss: {logs['loss']:.4f}\")\n",
        "        \n",
        "        print(\"Starting training...\")\n",
        "        for i in range(num_epochs):\n",
        "            print(f\"Epoch {i+1}/{num_epochs}\")\n",
        "            model.fit(\n",
        "                x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=1,\n",
        "                verbose=0,\n",
        "                callbacks=[ScoreLogger()]\n",
        "            )\n",
        "        \n",
        "        print(\"Training complete!\")\n",
        "        \n",
        "        # Evaluate the model\n",
        "        print(\"Evaluating model...\")\n",
        "        evaluation = model.evaluate(x_test, y_test, verbose=0)\n",
        "        test_loss, test_accuracy = evaluation\n",
        "        print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"Test loss: {test_loss:.4f}\")\n",
        "        \n",
        "        # End timing\n",
        "        end_training_time = time.time()\n",
        "        \n",
        "        # Calculate metrics\n",
        "        training_time_ms = (end_training_time - start_training_time) * 1000\n",
        "        peak_memory_mb = memory_tracker.get_total_peak_memory_mb()\n",
        "        \n",
        "        # Create memory profile JSON\n",
        "        memory_profile = {\n",
        "            \"summary\": memory_tracker.get_summary()\n",
        "        }\n",
        "        \n",
        "        # Create evaluation JSON with matching fields to Java version\n",
        "        evaluation_data = {\n",
        "            \"accuracy\": float(test_accuracy),\n",
        "            \"precision\": float(test_accuracy),  # Simplified - in real life we'd calculate actual precision\n",
        "            \"recall\": float(test_accuracy),     # Simplified - in real life we'd calculate actual recall\n",
        "            \"f1\": float(test_accuracy)          # Simplified - in real life we'd calculate actual F1\n",
        "        }\n",
        "        \n",
        "        return TrainingResult(training_time_ms, peak_memory_mb, float(test_accuracy), memory_profile, evaluation_data)\n",
        "\n",
        "\n",
        "def test_network(model, image_file):\n",
        "    \"\"\"Run inference test with detailed metrics like Java version\"\"\"\n",
        "    print(\"\\n--- Running inference test ---\")\n",
        "    \n",
        "    # Force garbage collection before starting\n",
        "    gc.collect()\n",
        "    time.sleep(0.1)  # Like Java's Thread.sleep(100)\n",
        "    \n",
        "    start_mem = get_used_memory()\n",
        "    process = psutil.Process(os.getpid())\n",
        "    start_cpu_time = process.cpu_times().user + process.cpu_times().system\n",
        "    \n",
        "    # Start memory tracking with 10ms intervals (like Java)\n",
        "    with MemoryTracker(interval_ms=10) as memory_tracker:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Perform actual inference\n",
        "        try:\n",
        "            # Load and preprocess image like Java's NativeImageLoader\n",
        "            img = Image.open(image_file).convert('L')  # Convert to grayscale\n",
        "            img = img.resize((28, 28))  # Resize to MNIST dimensions\n",
        "            \n",
        "            # Convert to numpy array and normalize like Java's ImagePreProcessingScaler\n",
        "            image = np.array(img, dtype=np.float32) / 255.0\n",
        "            \n",
        "            # Reshape to match the Java reshape(1, 28 * 28)\n",
        "            # In TF, we need to keep original dimensions but add batch dimension\n",
        "            image = image.reshape(1, 28, 28)\n",
        "            \n",
        "            # Run prediction\n",
        "            output = model.predict(image, verbose=0)\n",
        "            prediction = np.argmax(output[0])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image: {e}\")\n",
        "            traceback.print_exc()\n",
        "            prediction = -1\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "    # Calculate metrics\n",
        "    end_cpu_time = process.cpu_times().user + process.cpu_times().system\n",
        "    end_mem = get_used_memory()\n",
        "    \n",
        "    time_ms = (end_time - start_time) * 1000\n",
        "    memory_used_mb = (end_mem - start_mem) / 1024 / 1024\n",
        "    cpu_time_ms = (end_cpu_time - start_cpu_time) * 1000\n",
        "    peak_memory_mb = memory_tracker.get_total_peak_memory_mb()\n",
        "    \n",
        "    memory_profile = {\n",
        "        \"summary\": memory_tracker.get_summary()\n",
        "    }\n",
        "    \n",
        "    # Print results similar to Java output\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(f\"Execution time (wall): {time_ms:.2f} ms\")\n",
        "    print(f\"CPU time: {cpu_time_ms:.2f} ms\")\n",
        "    print(f\"Memory used (end-start): {memory_used_mb:.2f} MB\")\n",
        "    print(f\"Peak memory used: {peak_memory_mb:.2f} MB\")\n",
        "    print(f\"Peak heap memory: {memory_tracker.get_peak_heap_usage_mb():.2f} MB\")\n",
        "    print(f\"Peak virtual memory: {memory_tracker.get_peak_virtual_usage_mb():.2f} MB\")\n",
        "    \n",
        "    return InferenceResult(prediction, time_ms, memory_used_mb, cpu_time_ms, peak_memory_mb, memory_profile)\n",
        "\n",
        "\n",
        "def warmup_iteration(model, image_file):\n",
        "    \"\"\"Perform a warmup iteration like in Java\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_file).convert('L')\n",
        "        img = img.resize((28, 28))\n",
        "        image = np.array(img, dtype=np.float32) / 255.0\n",
        "        image = image.reshape(1, 28, 28)\n",
        "        model.predict(image, verbose=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Warmup error: {e}\")\n",
        "\n",
        "\n",
        "def get_system_info():\n",
        "    \"\"\"Get system information similar to Java's version\"\"\"\n",
        "    import multiprocessing\n",
        "    import sys\n",
        "    \n",
        "    memory = psutil.virtual_memory()\n",
        "    \n",
        "    return {\n",
        "        \"available_processors\": multiprocessing.cpu_count(),\n",
        "        \"max_memory_mb\": memory.total / (1024 * 1024),\n",
        "        \"total_memory_mb\": memory.total / (1024 * 1024),\n",
        "        \"os_name\": platform.system(),\n",
        "        \"os_version\": platform.version(),\n",
        "        \"os_arch\": platform.machine(),\n",
        "        \"python_version\": platform.python_version()\n",
        "    }\n",
        "\n",
        "\n",
        "def get_used_memory():\n",
        "    \"\"\"Get current memory usage in bytes\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    return process.memory_info().rss\n",
        "\n",
        "\n",
        "def average(values):\n",
        "    \"\"\"Calculate mean of values\"\"\"\n",
        "    return statistics.mean(values)\n",
        "\n",
        "\n",
        "def std_dev(values):\n",
        "    \"\"\"Calculate standard deviation of values\"\"\"\n",
        "    if len(values) <= 1:\n",
        "        return 0\n",
        "    return statistics.stdev(values)\n",
        "\n",
        "\n",
        "def mode(values):\n",
        "    \"\"\"Find the most common value\"\"\"\n",
        "    try:\n",
        "        return statistics.mode(values)\n",
        "    except statistics.StatisticsError:\n",
        "        # If there's no unique mode, return the first value\n",
        "        return values[0] if values else None\n",
        "\n",
        "\n",
        "def save_to_json(training_result, inference_results, avg_time, std_dev_time, \n",
        "                avg_memory, std_dev_memory, avg_peak_memory, std_dev_peak_memory, \n",
        "                common_prediction):\n",
        "    \"\"\"Save metrics to JSON - matches Java's format\"\"\"\n",
        "    \n",
        "    root = {}\n",
        "    \n",
        "    # Convert numpy types to native Python types\n",
        "    def convert_numpy_types(obj):\n",
        "        if isinstance(obj, (np.integer, np.floating)):\n",
        "            return float(obj) if isinstance(obj, np.floating) else int(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, dict):\n",
        "            return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, (list, tuple)):\n",
        "            return [convert_numpy_types(x) for x in obj]\n",
        "        return obj\n",
        "    \n",
        "    # Add timestamp in same format as Java\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3] + datetime.datetime.now().strftime(\"%z\")\n",
        "    root[\"timestamp\"] = timestamp\n",
        "    root[\"system_info\"] = convert_numpy_types(get_system_info())\n",
        "    \n",
        "    # Add training metrics (excluding timeseries)\n",
        "    training_obj = {\n",
        "        \"training_time_ms\": convert_numpy_types(training_result.training_time_ms),\n",
        "        \"training_time_seconds\": convert_numpy_types(training_result.training_time_ms / 1000.0),\n",
        "        \"peak_memory_mb\": convert_numpy_types(training_result.peak_memory_mb),\n",
        "        \"accuracy\": convert_numpy_types(training_result.accuracy),\n",
        "        \"memory_profile\": convert_numpy_types(training_result.memory_profile[\"summary\"]),\n",
        "        \"evaluation\": convert_numpy_types(training_result.evaluation)\n",
        "    }\n",
        "    root[\"training\"] = training_obj\n",
        "    \n",
        "    # Add inference runs (excluding timeseries)\n",
        "    runs_array = []\n",
        "    for i, r in enumerate(inference_results):\n",
        "        run_obj = {\n",
        "            \"run\": i + 1,\n",
        "            \"prediction\": convert_numpy_types(r.prediction),\n",
        "            \"execution_time_ms\": convert_numpy_types(r.time_ms),\n",
        "            \"cpu_time_ms\": convert_numpy_types(r.cpu_time_ms),\n",
        "            \"memory_used_mb\": convert_numpy_types(r.memory_used_mb),\n",
        "            \"peak_memory_mb\": convert_numpy_types(r.peak_memory_mb),\n",
        "            \"memory_profile\": convert_numpy_types(r.memory_profile[\"summary\"])\n",
        "        }\n",
        "        runs_array.append(run_obj)\n",
        "    \n",
        "    root[\"inference_runs\"] = runs_array\n",
        "    \n",
        "    root[\"inference_summary\"] = {\n",
        "        \"average_execution_time_ms\": convert_numpy_types(avg_time),\n",
        "        \"std_dev_execution_time_ms\": convert_numpy_types(std_dev_time),\n",
        "        \"average_memory_used_mb\": convert_numpy_types(avg_memory),\n",
        "        \"std_dev_memory_used_mb\": convert_numpy_types(std_dev_memory),\n",
        "        \"average_peak_memory_mb\": convert_numpy_types(avg_peak_memory),\n",
        "        \"std_dev_peak_memory_mb\": convert_numpy_types(std_dev_peak_memory),\n",
        "        \"most_common_prediction\": convert_numpy_types(common_prediction)\n",
        "    }\n",
        "    \n",
        "    with open(\"medium_network_results_python256.json\", \"w\") as f:\n",
        "        json.dump(root, f, indent=4)\n",
        "    \n",
        "    print(\"Results exported to medium_network_results_python.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCb2HcY_j2mC",
        "outputId": "edb67ba0-eb71-4f3c-b12d-e5841fafd471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_5 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,386\n",
            "Trainable params: 109,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Loading data...\n",
            "Starting training...\n",
            "Epoch 1/10\n",
            "Batch 0, loss: 2.3870\n",
            "Batch 100, loss: 0.6508\n",
            "Batch 200, loss: 0.4560\n",
            "Epoch 2/10\n",
            "Batch 0, loss: 0.1584\n",
            "Batch 100, loss: 0.1809\n",
            "Batch 200, loss: 0.1701\n",
            "Epoch 3/10\n",
            "Batch 0, loss: 0.0982\n",
            "Batch 100, loss: 0.1318\n",
            "Batch 200, loss: 0.1211\n",
            "Epoch 4/10\n",
            "Batch 0, loss: 0.0876\n",
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0001s vs `on_train_batch_end` time: 0.0011s). Check your callbacks.\n",
            "Batch 100, loss: 0.0943\n",
            "Batch 200, loss: 0.0921\n",
            "Epoch 5/10\n",
            "Batch 0, loss: 0.1104\n",
            "Batch 100, loss: 0.0726\n",
            "Batch 200, loss: 0.0727\n",
            "Epoch 6/10\n",
            "Batch 0, loss: 0.0557\n",
            "Batch 100, loss: 0.0633\n",
            "Batch 200, loss: 0.0611\n",
            "Epoch 7/10\n",
            "Batch 0, loss: 0.0326\n",
            "Batch 100, loss: 0.0519\n",
            "Batch 200, loss: 0.0502\n",
            "Epoch 8/10\n",
            "Batch 0, loss: 0.0416\n",
            "Batch 100, loss: 0.0440\n",
            "Batch 200, loss: 0.0439\n",
            "Epoch 9/10\n",
            "Batch 0, loss: 0.0371\n",
            "Batch 100, loss: 0.0351\n",
            "Batch 200, loss: 0.0359\n",
            "Epoch 10/10\n",
            "Batch 0, loss: 0.0463\n",
            "Batch 100, loss: 0.0267\n",
            "Batch 200, loss: 0.0302\n",
            "Training complete!\n",
            "Evaluating model...\n",
            "Test accuracy: 0.9766\n",
            "Test loss: 0.0765\n",
            "\n",
            "===== Training Metrics =====\n",
            "Total training time: 6805.72 ms (6.81 seconds)\n",
            "Peak training memory: 1924.68 MB\n",
            "Final model accuracy: 97.66%\n",
            "\n",
            "Performing warm-up iterations...\n",
            "Warm-up complete, starting benchmark...\n",
            "\n",
            "--- Test Run 1 ---\n",
            "\n",
            "--- Running inference test ---\n",
            "Prediction: 2\n",
            "Execution time (wall): 48.71 ms\n",
            "CPU time: 140.62 ms\n",
            "Memory used (end-start): 0.03 MB\n",
            "Peak memory used: 431.87 MB\n",
            "Peak heap memory: 431.87 MB\n",
            "Peak virtual memory: 409.21 MB\n",
            "\n",
            "--- Test Run 2 ---\n",
            "\n",
            "--- Running inference test ---\n",
            "Prediction: 2\n",
            "Execution time (wall): 48.82 ms\n",
            "CPU time: 171.88 ms\n",
            "Memory used (end-start): 0.44 MB\n",
            "Peak memory used: 432.11 MB\n",
            "Peak heap memory: 432.11 MB\n",
            "Peak virtual memory: 409.47 MB\n",
            "\n",
            "--- Test Run 3 ---\n",
            "\n",
            "--- Running inference test ---\n",
            "Prediction: 2\n",
            "Execution time (wall): 48.76 ms\n",
            "CPU time: 140.62 ms\n",
            "Memory used (end-start): 0.02 MB\n",
            "Peak memory used: 432.33 MB\n",
            "Peak heap memory: 432.33 MB\n",
            "Peak virtual memory: 409.65 MB\n",
            "\n",
            "--- Test Run 4 ---\n",
            "\n",
            "--- Running inference test ---\n",
            "Prediction: 2\n",
            "Execution time (wall): 48.72 ms\n",
            "CPU time: 78.12 ms\n",
            "Memory used (end-start): 0.01 MB\n",
            "Peak memory used: 432.33 MB\n",
            "Peak heap memory: 432.33 MB\n",
            "Peak virtual memory: 409.65 MB\n",
            "\n",
            "--- Test Run 5 ---\n",
            "\n",
            "--- Running inference test ---\n",
            "Prediction: 2\n",
            "Execution time (wall): 49.60 ms\n",
            "CPU time: 218.75 ms\n",
            "Memory used (end-start): 0.01 MB\n",
            "Peak memory used: 432.32 MB\n",
            "Peak heap memory: 432.32 MB\n",
            "Peak virtual memory: 409.64 MB\n",
            "\n",
            "===== Average Inference Results After 5 Runs =====\n",
            "Most common prediction: 2\n",
            "Average execution time: 48.92 ms (±0.38)\n",
            "Average memory used: 0.10 MB (±0.19)\n",
            "Average peak memory: 432.19 MB (±0.20)\n",
            "Results exported to medium_network_results_python.json\n",
            "Model saved to models/trained-medium-model.h5\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Create a directory for models if it doesn't exist\n",
        "    Path(\"models\").mkdir(exist_ok=True)\n",
        "    \n",
        "    # Create the network\n",
        "    model = create_medium_network()\n",
        "    model.summary()\n",
        "    \n",
        "    # Train the network and measure training metrics\n",
        "    training_result = train_network_with_metrics(model)\n",
        "    \n",
        "    print(\"\\n===== Training Metrics =====\")\n",
        "    print(f\"Total training time: {training_result.training_time_ms:.2f} ms ({training_result.training_time_ms / 1000.0:.2f} seconds)\")\n",
        "    print(f\"Peak training memory: {training_result.peak_memory_mb:.2f} MB\")\n",
        "    print(f\"Final model accuracy: {training_result.accuracy * 100:.2f}%\")\n",
        "    \n",
        "    # Path to test image\n",
        "    image_file = \"träningsbilder/testSample/img_1.jpg\"\n",
        "    \n",
        "    # Add explicit warm-up phase\n",
        "    print(\"\\nPerforming warm-up iterations...\")\n",
        "    for i in range(10):\n",
        "        warmup_iteration(model, image_file)\n",
        "    print(\"Warm-up complete, starting benchmark...\")\n",
        "    \n",
        "    # Force garbage collection before testing\n",
        "    gc.collect()\n",
        "    time.sleep(0.5)\n",
        "    \n",
        "    # Run multiple tests like Java version\n",
        "    runs = 5\n",
        "    inference_results = []\n",
        "    \n",
        "    for i in range(runs):\n",
        "        print(f\"\\n--- Test Run {i + 1} ---\")\n",
        "        result = test_network(model, image_file)\n",
        "        inference_results.append(result)\n",
        "    \n",
        "    # Aggregate metrics\n",
        "    avg_time = average([r.time_ms for r in inference_results])\n",
        "    std_dev_time = std_dev([r.time_ms for r in inference_results])\n",
        "    \n",
        "    avg_memory = average([r.memory_used_mb for r in inference_results])\n",
        "    std_dev_memory = std_dev([r.memory_used_mb for r in inference_results])\n",
        "    \n",
        "    avg_peak_memory = average([r.peak_memory_mb for r in inference_results])\n",
        "    std_dev_peak_memory = std_dev([r.peak_memory_mb for r in inference_results])\n",
        "    \n",
        "    common_prediction = mode([r.prediction for r in inference_results])\n",
        "    \n",
        "    print(\"\\n===== Average Inference Results After 5 Runs =====\")\n",
        "    print(f\"Most common prediction: {common_prediction}\")\n",
        "    print(f\"Average execution time: {avg_time:.2f} ms (±{std_dev_time:.2f})\")\n",
        "    print(f\"Average memory used: {avg_memory:.2f} MB (±{std_dev_memory:.2f})\")\n",
        "    print(f\"Average peak memory: {avg_peak_memory:.2f} MB (±{std_dev_peak_memory:.2f})\")\n",
        "    \n",
        "    # Save metrics to JSON\n",
        "    save_to_json(training_result, inference_results, avg_time, std_dev_time, \n",
        "                avg_memory, std_dev_memory, avg_peak_memory, std_dev_peak_memory, \n",
        "                common_prediction)\n",
        "    \n",
        "    # Save the model\n",
        "    model.save(\"models/trained-medium-model.h5\")\n",
        "    print(\"Model saved to models/trained-medium-model.h5\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOBoch9kIhmjmeVzR8S5ZNQ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "exJobbEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
